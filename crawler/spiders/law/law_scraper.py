import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import json
import re
import argparse
import os
from urllib.parse import urlparse, parse_qs

def clean_spaces(s: str) -> str:
    """í…ìŠ¤íŠ¸ì˜ ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ ì¤„ë°”ê¿ˆì„ ì •ë¦¬í•©ë‹ˆë‹¤."""
    s = (s or "").replace("\xa0", " ")
    # â­ï¸ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ ë¡œì§ì„ ì•½ê°„ ìˆ˜ì •í•˜ì—¬ ì—°ì†ëœ ì¤„ë°”ê¿ˆë§Œ ì •ë¦¬í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s)
    return s.strip()

def parse_law_html(html: str, url: str):
    """
    ë²•ë ¹ì˜ 'ë¬¸ì„œ' ì •ë³´ì™€ ê° 'ì¡°'ì— í•´ë‹¹í•˜ëŠ” 'ì²­í¬' ì •ë³´ë¥¼ ë¶„ë¦¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    soup = BeautifulSoup(html, 'html.parser')

    doc_title_tag = soup.select_one('#conTop h2')
    doc_title = doc_title_tag.get_text(strip=True) if doc_title_tag else ""

    doc_subtitle_tag = soup.select_one('div.ct_sub')
    doc_subtitle = doc_subtitle_tag.get_text(strip=True) if doc_subtitle_tag else ""

    parsed_url = urlparse(url)
    query_params = parse_qs(parsed_url.query)
    doc_id_keys = ['lsiSeq', 'lsId']
    doc_id = next((query_params.get(key, [None])[0] for key in doc_id_keys if query_params.get(key)), None)

    if not doc_id:
        print("âš ï¸ ê²½ê³ : URLì—ì„œ ë¬¸ì„œ ID(lsiSeq, lsId)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        doc_id = re.sub(r'[^a-zA-Z0-9]', '', doc_title)

    document_data = {
        "doc_id": doc_id,
        "title": doc_title,
        "subtitle": doc_subtitle,
        "source_url": url
    }

    content_div = soup.select_one('#contentBody, #conScroll, .law-view-content')
    if not content_div:
        print("âŒ ì—ëŸ¬: ë²•ë¥  ë‚´ìš©ì´ ë‹´ê¸´ ë©”ì¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return document_data, []

    chunks = []
    current_chapter = ""

    for pgroup_div in content_div.select('div.pgroup'):
        chapter_tag = pgroup_div.select_one('p.gtit')
        if chapter_tag:
            chapter_text = chapter_tag.get_text(strip=True)
            if re.match(r'^ì œ\s*\d+\s*ì¥', chapter_text):
                current_chapter = chapter_text

        title_tag = pgroup_div.select_one('span.bl label')
        if not title_tag:
            continue
        article_title = title_tag.get_text(strip=True)

        article_id_num = "".join(filter(str.isdigit, article_title.split('(')[0]))
        if not article_id_num:
            continue

        # â­ï¸ [í•µì‹¬ ìˆ˜ì •] <a> íƒœê·¸ë¥¼ <span>ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ í…ìŠ¤íŠ¸ê°€ ì´ì–´ì§€ë„ë¡ ì²˜ë¦¬
        for a_tag in pgroup_div.find_all('a'):
            a_tag.unwrap() # a íƒœê·¸ë¥¼ ì œê±°í•˜ê³  ë‚´ìš©ë¬¼ë§Œ ë‚¨ê¹€ (unwrap)

        # í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ë•Œ, separatorë¥¼ ê³µë°±ìœ¼ë¡œ ì£¼ì–´ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°ë˜ë„ë¡ í•¨
        full_text = pgroup_div.get_text(separator="", strip=True)
        # ì´í›„ ì •ì œ ê³¼ì •ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±ì„ ì •ë¦¬í•˜ê³  ì˜ë¯¸ìˆëŠ” ì¤„ë°”ê¿ˆì€ ìœ ì§€

        # ì •ì œë¥¼ ìœ„í•´ BeautifulSoup ê°ì²´ë¥¼ ë‹¤ì‹œ ì‚¬ìš©
        temp_soup = BeautifulSoup(str(pgroup_div), 'html.parser')
        for tag in temp_soup.find_all(['p', 'br']):
            tag.append('\n')

        text_with_newlines = temp_soup.get_text(separator="")
        final_text = "\n".join(line.strip() for line in text_with_newlines.splitlines() if line.strip())


        chunk_data = {
            "chunk_id": f"doc:{doc_id}:article_{article_id_num}",
            "doc_id": doc_id,
            "title": article_title,
            "text": final_text, # ì •ì œëœ í…ìŠ¤íŠ¸ ì‚¬ìš©
            "metadata": {"chapter": current_chapter},
            "source_url": url
        }
        chunks.append(chunk_data)

    return document_data, chunks

def save_to_file(data, filename):
    """ë°ì´í„°ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    if not isinstance(data, list):
        data = [data]

    directory = os.path.dirname(filename)
    if directory and not os.path.exists(directory):
        os.makedirs(directory)
        print(f"ğŸ“ '{directory}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

    with open(filename, 'w', encoding='utf-8') as f:
        for item in data:
            f.write(json.dumps(item, ensure_ascii=False) + '\n')
    print(f"âœ… ë°ì´í„° {len(data)}ê±´ì„ '{filename}' íŒŒì¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

async def main(url: str, output_name: str):
    """Playwrightë¥¼ ì‹¤í–‰í•˜ì—¬ ì›¹í˜ì´ì§€ ì»¨í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            print(f"í˜ì´ì§€ë¡œ ì´ë™ ì¤‘: {url}")
            await page.goto(url, wait_until='domcontentloaded', timeout=60000)

            try:
                print("ë¡œë”© ë§ˆìŠ¤í¬ê°€ ë‚˜íƒ€ë‚˜ê¸°ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
                await page.wait_for_selector(".loadmask-msg", state="visible", timeout=10000)
                print("ë¡œë”© ë§ˆìŠ¤í¬ê°€ ì‚¬ë¼ì§€ê¸°ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...")
                await page.wait_for_selector(".loadmask-msg", state="hidden", timeout=30000)
            except Exception:
                print("âš ï¸ ë¡œë”© ë§ˆìŠ¤í¬ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í˜ì´ì§€ ë¡œë“œê°€ ì™„ë£Œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

            print("í˜ì´ì§€ ìµœì¢… ë Œë”ë§ì„ ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
            await page.wait_for_timeout(1000)

            print("í˜ì´ì§€ HTML ì»¨í…ì¸ ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            html = await page.content()

            print("ë°ì´í„° íŒŒì‹± ì¤‘...")
            doc_data, chunk_data = parse_law_html(html, url)

            doc_filename = os.path.join('../export', f'{output_name}_document.jsonl')
            chunk_filename = os.path.join('../export', f'{output_name}_chunks.jsonl')

            if doc_data.get("title"):
                save_to_file(doc_data, doc_filename)
            else:
                print("âš ï¸ ê²½ê³ : ë¬¸ì„œ ì œëª©ì„ ì°¾ì§€ ëª»í•´ document íŒŒì¼ì„ ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            if chunk_data:
                save_to_file(chunk_data, chunk_filename)
            else:
                print("âš ï¸ ê²½ê³ : í˜ì´ì§€ì—ì„œ ì²­í¬(ì¡°ë¬¸) ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë ˆì´í•‘ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
            error_dir = '../debug'
            if not os.path.exists(error_dir):
                os.makedirs(error_dir)
            screenshot_path = os.path.join(error_dir, f'{output_name}_error.png')
            html_path = os.path.join(error_dir, f'{output_name}_error.html')

            await page.screenshot(path=screenshot_path)
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(await page.content())

            print(f"ğŸ“¸ ì—ëŸ¬ ë°œìƒ ì‹œì ì˜ ìŠ¤í¬ë¦°ìƒ·: {screenshot_path}")
            print(f"ğŸ“„ ì—ëŸ¬ ë°œìƒ ì‹œì ì˜ HTML: {html_path}")
        finally:
            await browser.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„° ë²•ë ¹ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë ˆì´í•‘í•˜ì—¬ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.")
    parser.add_argument("url", help="ìŠ¤í¬ë ˆì´í•‘í•  ë²•ë ¹ í˜ì´ì§€ì˜ ì „ì²´ URL")
    parser.add_argument("-o", "--output", required=True, help="ì¶œë ¥ íŒŒì¼ì˜ ê¸°ë³¸ ì´ë¦„ (ì˜ˆ: gasa-law)")

    args = parser.parse_args()

    asyncio.run(main(args.url, args.output))